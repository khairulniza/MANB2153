{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Khairulniza Bin Ibrahim \n",
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Classification Using Naive Bayes and Decison Tree On Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/khair/Desktop/MScBIA/Sem 1/Data Mining/iris.csv',delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5.1</th>\n",
       "      <th>3.5</th>\n",
       "      <th>1.4</th>\n",
       "      <th>0.2</th>\n",
       "      <th>Iris-setosa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   5.1  3.5  1.4  0.2  Iris-setosa\n",
       "0  4.9  3.0  1.4  0.2  Iris-setosa\n",
       "1  4.7  3.2  1.3  0.2  Iris-setosa\n",
       "2  4.6  3.1  1.5  0.2  Iris-setosa\n",
       "3  5.0  3.6  1.4  0.2  Iris-setosa\n",
       "4  5.4  3.9  1.7  0.4  Iris-setosa"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_input = df.iloc[:,0:4]\n",
    "iris_target = df.iloc[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_YY = []\n",
    "for cat in iris_target:\n",
    "    if cat == 'Iris-setosa':\n",
    "        cls = 1\n",
    "    elif  cat == 'Iris-versicolor':\n",
    "        cls = 2\n",
    "    else:\n",
    "        cls = 3\n",
    "        \n",
    "    iris_YY.append(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X = np.asarray(iris_input)\n",
    "iris_y = np.asarray(iris_YY)\n",
    "\n",
    "np.random.seed(0)\n",
    "indices = np.random.permutation(len(iris_X))\n",
    "\n",
    "iris_X_train = iris_X[indices[:-10]]\n",
    "iris_y_train = iris_y[indices[:-10]]\n",
    "iris_X_test  = iris_X[indices[-10:]]\n",
    "iris_y_test  = iris_y[indices[-10:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes classifier on training data is:0.9459459459459459\n",
      "The accuracy of the Naive Bayes classifier on testing data is: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB \n",
    "\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y)\n",
    "NB = GaussianNB() \n",
    "NB.fit(iris_X_train, iris_y_train)\n",
    "print(\"The accuracy of the Naive Bayes classifier on training data is:\", NB.score(iris_X_train, iris_y_train) , sep = \"\")\n",
    "print(\"The accuracy of the Naive Bayes classifier on testing data is: \", NB.score(iris_X_test, iris_y_test) , sep = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Decison Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "decision_tree = tree.DecisionTreeClassifier(criterion='gini')\n",
    "decision_tree.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Decision Tree classifier on training data is: 1.00\n",
      "The accuracy of the Decision Tree classifier on test data is: 1.00\n"
     ]
    }
   ],
   "source": [
    "#Print performance\n",
    "print('The accuracy of the Decision Tree classifier on training data is: {:.2f}'.format(decision_tree.score(iris_X_train, iris_y_train)))\n",
    "print('The accuracy of the Decision Tree classifier on test data is: {:.2f}'.format(decision_tree.score(iris_X_test, iris_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "### kNN classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 2, 3, 1, 3, 3, 2, 1, 2, 3, 2, 2, 2, 1, 3, 1, 2, 2, 2, 2, 1,\n",
       "       3, 2, 3, 1, 3, 2, 3, 2, 3, 2, 3, 3, 1, 2, 3, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(iris_X_train, iris_y_train) \n",
    "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
    "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
    "           weights='uniform')\n",
    "predict_result_iris = knn.predict(iris_X_test)\n",
    "iris_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Knn classifier on training data is: 0.95\n",
      "The accuracy of the Knn classifier on test data is: 1.00\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of the Knn classifier on training data is: {:.2f}'.format(knn.score(iris_X_train, iris_y_train)))\n",
    "print('The accuracy of the Knn classifier on test data is: {:.2f}'.format(knn.score(iris_X_test, iris_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Journal Review on Classification Techniques (NB, DT and kNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 1: Text Classification using KNN with different Features Selection Methods [1]\n",
    ">Researcher is focusing on application of kNN classifier for feature selection technique in the area of text classifications. kNN through the literiture review has shown a faster and efficient approach as compared to NaÃ¯ve Bayes, Perceptron, Random Forest and Stochastic Gradient Descent. The other reason why kNN was selected is also because of the kNN is capable to handle (classify) a bigger size douments as compared to other classifers. \n",
    "\n",
    ">The feature selection method that is used in this research are the chi-squared method, information gain and mutual information. All of the features selection techniques are compared against each other by looking to the accuracy of the technique to classify the given data using the kNN method.\n",
    "\n",
    ">By using kNN, each new document is compared to fundamental documents. This Algorithm checks how a document is classified by looking at only training data that are equal to it. KNN assumes that to divide the documents like a points in the Euclidean area\n",
    "\n",
    ">From the conclusion chapter, kNN Classifier is found to be works better for less number of  features. According to the resercher, as we increase number of feature number the performance of KNN Classifier decreases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 2: Prediction of Student's Performance Using Modified kNN Classifiers [2]\n",
    "\n",
    ">Researchers aim to predict the studentâs performance according to their marks through classification using modified kNN classifiers such as *Cosine kNN, Cubic kNN,* and *Weighted kNN*.\n",
    "\n",
    ">The reserchers believed that early prediction of the studentâs grade can help the principals to take decisions in order to help schools to identify students with low academic achievement and find ways to support them as soon as possible. \n",
    "\n",
    ">Reserchers perform prediction by comparing the input marks with the training dataset and find the nearest value of marks and assign a class of the student marks. \n",
    "\n",
    ">As mentioned by the reserchers, every algorithm gives a different value of accuracy and predictions due to the distance metrics used. Distance metrics, distance weight and number of neighbors are changeable variables that lead to enhance the efficiency of the algorithm results by increasing the accuracy of classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 3: A New Intrusion Detection System Based on kNN Classification Algorithm in Wireless Sensor Network [3]\n",
    "\n",
    ">Researchers aim to propose a new intrusion detection system based on kNN classification algorithm in the area of wireless sensor network in order to detetect intrusion. \n",
    "\n",
    ">This to protect the wireless sensor; which is open nature of the information media and the poor deployment environment that brought great risks to the security of wireless sensor networks.  \n",
    "\n",
    ">According to them, that is even critical in the Internet of Things (IoT) evironemnt which predominatly involved wireless sensor network. This wireless network is facing security threats. Some of the main threats are *Dos attack, replay attack, integrity attack, false routing information attack, and flooding attack*. But in this paper, researchers are focing only a single threat which is the *flooding attack*.\n",
    "\n",
    ">Reseracher use the kNN classifier to classify and separate abnormal nodes from normal nodes by observing their abnormal behaviors, analyse parameter selection and error rate of the intrusion detection system.\n",
    "\n",
    ">As a result, researchers proof that the system which uses the kNN classifer is deemed able to detect *flooding attack* in wireless sensor network and met thier research objective.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 4: Breast Cancer Prediction using NaÃ¯ve Bayes Classifier [4]\n",
    "\n",
    ">Researcher is exploring to develop a predictive model using NaÃ¯ve Bayes classifier. The predictive model will contribute significantly in order to identify the type of breast cancer: *benign* or *malignant* which will help oncologist in diagnosing the cancer type with shortest time possible and then helps oncologist to prescribe treatment method.\n",
    "\n",
    ">As breast cancer is recorded as major cause of death among women, early detection will help the patients and doctors to classify the type as well as the treatment method. An analysis has shown that survival rate is 88% after 5 years of diagnosis and 80% after 10 years of diagnosis.Therefore it is necessary to detect breast cancer at earliest stage possible\n",
    "\n",
    ">NaÃ¯ve Bayes algorithm is use to predict cancer type, research uses JAVA Netbeans interface and compared the result with the other algorithm using WEKA.  The confusion matrix is used to analyze the performance criterion for the classifiers in detecting breast cancer; accuracy, precision (for multiclass dataset), sensitivity and specificity for algorithm decision.\n",
    "\n",
    ">In this study, result shows that breast cancer classification is much better if it is implemented using NaÃ¯ve Bayes algorithm as compared to other machine learning algorithm â *SMO, Bayes Network,J-48 Decision*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 5: Using NaÃ¯ve Bayes Algorithm in detection of Hate Tweets [5]\n",
    "\n",
    ">Researchers aim to develop a reliable tool for detecting and classifying hateful speech that uses content produced by self-identifying hateful communities from Twitter by using NaÃ¯ve Bayes classifier.\n",
    "\n",
    ">As we know, online spaces are often exploited and misused to spread content that can be degrading, abusive, or otherwise harmful to people. Twitter has regulationn rules that prohibits users to post violent threats, harassment, and hateful contents. However, there are still tons of users who disobey the rules and use their Twitter account to spread hate speech and negative words\n",
    "\n",
    ">Researchers use the approach that involves tweet acquisition and streaming using Tweepy API, pre-processing to remove unwanted parts of speech using n-grams, and tweet classification and evaluation (using NaÃ¯ve Bayes).\n",
    "\n",
    ">Results show that the classifiers yielded better results for the hate tweets review with the NaÃ¯ve Bayes. It gives more than 80% of accuracies and outperforming other algorithms for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 6: Diagnosis of Power Transformer Oil Using KNN and NaÃ¯ve Bayes Classifiers [6]\n",
    "\n",
    ">Researchers aim to enhance dissolved gas analysis (DGA) analysis using the machine learning technique. DGA is usually used for monitoring faults in transformer oil insulating. The gases dissolved in the insulating oil is currently evaluated (measure in ppm level - parts per million) by using online mode or offline mode (at laboratory).\n",
    "\n",
    ">Transformer Oil is used to insulate, suppress corona discharge and arcing, and to serve as a coolant to the transformer.The repercussion due to instability of power transformer affects not only the availability in the electrical energy, but also the technical-economic penalties.\n",
    "\n",
    ">This research uses kNN and NaÃ¯ve Bayes classifiers were used and compared in order to diagnosing the insulating oil. Both were employed to diagnosis of insulating oil used in power transformer by analysing five input vectors have been adopted which consist of DGA in ppm, DGA in percentage, Duval triangle reports, Rogers ratios, Dornrenberg ratios for gas dissolved in the insulating oil *hydrogen (H2), methane (CH4), acetylene (C2H2), ethylene (C2H4), ethane (C2H6), carbon monoxide (CO)* and *carbon dioxide (CO2)*\n",
    "\n",
    ">It was found that the kNN algorithm provides the highest accuracy rate as compared to NaÃ¯ve Bayes in all five input vectors. kNN results ranging from 70%-92% of accuracy whereby NaÃ¯ve Bayes ranging from 32%-86% of accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 7: Analyzing brain signals using decision trees: an approach based on neuroscience [7]\n",
    "\n",
    ">Researcher aims to analyze brain signals to better understand the brain and its functionalities. Data were collected with *Emotiv Epoc* (is a device that allows monitoring of 14 channels (beyondthe landmarks CMS/DRL - Common Mode Sense/Driven Right Leg, terminals P3/P4 and channels of signal quality) and using decision trees for the classification. Decison tree that were generated were analysed and exlained using neuroscience knowledge. \n",
    "\n",
    ">There is a part of population that they may need other ways to interact with electronic equipment due to health problems or accidents, that they lead to severe paralysis. With the development of Brain Computer Interfaces (BCI) which aims to provide this communication without the use of muscle movements. The researcher is exploring signal pattern between real time activity as compared to by memory activity. \n",
    "\n",
    ">Data collected from 3 subjects which all of them at 13 years-old and analysed using WEKA software, configured to *J48* algorithm. Data collection was performed during 4 consecutive days, using the equipment *Emotiv EPOC*. The subjects were oriented to open and close the hand holding the ball for a minute, first with the left hand and then with the right hand. They were oriented to imagine the same task. Between each task (do and imagine) there was a pause of approximately 30 seconds. Data from the imagination task were used for analysis. \n",
    "\n",
    ">At the end the study, four decision trees: one for each collected day produced results are totally different in each day. Because the signals were collected only for 4 days, without previous training of the subject and without an adaptation period to the tasks (these tasks are not daily life tasks). This was explain by neuroscience theory , if the subject was âtrainedâ for a long period of time for an action (for example, more than 30 days), it becomes an automatic action, meaning the use of the same brain regions (generating decision trees very similar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 8: An Application of Decision Tree Based on ID3 [8]\n",
    "\n",
    ">Reserchers aim to explore the potential use of decison tree to identify potential buyers from thier purchase records using *individual mining generated rule base* for promotional or paid targeted advertising of the merchants.\n",
    "\n",
    ">Another objective is to address the online sales that rely on buyers browsing sellers' goods online, or through some\n",
    "advertising websites to increase sales which it can not accurately correspond to the target groups. This resulting bottlenecks in sales. \n",
    "\n",
    ">Data was obtained from most popular website and data mining decision tree classification was used to analyze the data using decison tree, ID3.\n",
    "\n",
    ">In the conclusion, the mining results were found not satisfactory, as full information about the users are not not complete. This might be a result from the user that is reluctant to share thier complete information or not using thier real information. Reserchers also mentioned that data mining tools can uncover more useful rules that are easy to understand. But for online users, data should be considered from different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 9: Decision Tree Classifiers in Bioinformatics [9]\n",
    "\n",
    ">Research is aims to solve the cancer classification problem using single decision tree classifiers (ie: algorithms C4.5 and CART) and decision tree forests (e.g. random forests) and compare the strengths and weaknesses of the proposed methodologies in comparison to other popular classification methods.\n",
    "\n",
    ">As mentioned in the paper, the choice of the classification method is not definite and different classification algorithms fit different problems â there is no dominant method. Furthermore, according to the researchers, decision trees are proven to be as effective as other classifiers and exceed the efficiency of other classifiers for particular problems. \n",
    "\n",
    "> This paper basically presents a literature review of articles related to the use of decision tree classifiers in the field of gene microarray data analysis that were published in the last ten years\n",
    "\n",
    "> Acording to the paper, previous researchers also give preference to decision tree classifiers because of their ability of relevant gene selection and scalability, as well as model accuracy and easy interpretation. \n",
    "\n",
    ">In the conclusion chapter, researchers conclude that with right data pre-processing, decision trees and their ensembles can be very efficient and outperform other methods. Better results can be achieved using aggregation of decision tree classifiers like bagging, boosting and Random forests.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Journal 10: Decision Tree Classification of Remotely Sensed Satellite Data using Spectral Separability Matrix [10]\n",
    "\n",
    "> As image classification is one of the primary tasks in geo-computation, that being used to categorize for further analysis such as land management, potential mapping, forecast analysis, soil assessment and etc image classification is method by which labels or class identifiers are attached to individual pixels on basis of their characteristics.This paper attempt to develop a decision tree classification algorithm for remotely sensed satellite data using the separability matrix of the spectral distributions of probable classes in the respective bands.\n",
    "\n",
    ">According to the researchers, non-parametric classification techniques such as Artificial Neural Network (ANN) and Rule-based classifiers are increasingly being used. By far, decison tree however not been used widely by the remote sensing community for land use classification despite their non-parametric nature and their attractive properties of simplicity, flexibility, and computational efficiency in handling the non-normal, non-homogeneous and noisy data, as well as non-linear relations between features and classes, missing values, and both numeric and categorical inputs. \n",
    "\n",
    ">Decision tree algorithm were coded in Visual C++ 6.0 language to develop user-friendly software for classification that requires a bitmap image of the area of interest as the basic input for the classification of remotely sensed satellite data using the separability matrix of spectral distributions of probable classes.\n",
    "\n",
    ">a decision tree classification algorithm for remotely sensed satellite data using separability matrix of spectral distributions of probable classes has been developed. The overall accuracy for the sample test image was found to be 98% using the Decision Tree method and 95% using the Maximum Likelihood method with kappa values 97% and 94 % respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference:\n",
    "    \n",
    "    [1] https://www.researchgate.net/publication/326893075_Text_Classification_using_KNN_with_different_Feature_Selection_Methods\n",
    "    \n",
    "    [2] https://www.researchgate.net/publication/324941826_Prediction_of_Student's_Performance_Using_Modified_KNN_Classifiers\n",
    "    \n",
    "    [3] https://www.researchgate.net/publication/275071876_A_New_Intrusion_Detection_System_Based_on_KNN_Classification_Algorithm_in_Wireless_Sensor_Network\n",
    "    \n",
    "    [4] https://www.researchgate.net/publication/308934053_Breast_Cancer_Prediction_using_Naive_Bayes_Classifier\n",
    "    \n",
    "    [5] https://www.researchgate.net/publication/323611781_Using_Naive_Bayes_Algorithm_in_detection_of_Hate_Tweets\n",
    "    \n",
    "    [6] https://www.researchgate.net/publication/327837626_Diagnosis_of_Power_Transformer_Oil_Using_KNN_and_Naive_Bayes_Classifiers\n",
    "    \n",
    "    [7] https://www.researchgate.net/publication/312071233_Analyzing_brain_signals_using_decision_trees_an_approach_based_on_neuroscience\n",
    "    \n",
    "    [8] https://www.sciencedirect.com/science/article/pii/S1875389212006098\n",
    "    \n",
    "    [9] https://www.researchgate.net/publication/220625930_Decision_Tree_Classifiers_in_Bioinformatics\n",
    "    \n",
    "    [10] https://www.researchgate.net/publication/49587771_Decision_Tree_Classification_of_Remotely_Sensed_Satellite_Data_using_Spectral_Separability_Matrix\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
